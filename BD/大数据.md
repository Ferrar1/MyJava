## 串起来

<img src="https://github.com/xuzhuang1996/MyJava/blob/master/img/BD/Hadoop.png" width=150% height=150% />

## 背景
1. MapReduce编程模型
   1. MapReduce采用”分而治之”的思想，把对大规模数据集的操作，分发给一个主节点管理下的各个分节点共同完成，然后通过整合各个节点的中间结果，得到最终结果。
   2. map负责把任务分解成多个任务，reduce负责把分解后多任务处理的结果汇总起来。
2. 例子：统计一系列文档中的词频，文档数量规模很大，有1000万个文档，英文单词的总数可能只有3000（常用的）。那么input M=10000000，output N=3000
   1. 搞了10000个PC做Mapper，100个PC做Reducer。
   2. 每个Mapper做1000个文档的词频统计，统计之后把凡是和同一个word相关的统计中间结果传给同一个Reducer做汇总。
   3. 比如某个Reducer负责词表中前30个词的词频统计，遍历10000个PC，这10000个Mapper PC把各自处理后和词表中前30个词汇相关的中间结果都传给这个Reducer做最终的处理分析
## 安装
1. linux下，如果不知道java安装目录，`echo $JAVA_HOME`。
2. windows下可以[参考](http://www.cnblogs.com/chevin/p/9090683.html)
   - 第一步，etc/hadoop/hadoop-env.cmd文件进行编辑，将`set JAVA_HOME=%JAVA_HOME%`改成`set JAVA_HOME=C:"\Program Files"\Java\jdk1.8.0_191`,然后才能在shell下执行bin/hadoop。这里有几个问题：
     - Windows下跟linux不同，linux也许是etc/hadoop/hadoop-env.sh，但是Windows下是etc/hadoop/hadoop-env.cmd文件。
     - 由于我的java装在C:\Program Files下，但是Program Files文件夹有空格，因此报错。解决：有几种方式，一种给Program Files加引号，或者改为C:\PROGRA~1
   - 第二步，独立操作，复制解压缩的conf目录以用作输入，然后查找并显示给定正则表达式的每个匹配项，输出将写入给定的输出目录。就是通过Hadoop做一个大数据的查找功能。不过应该会报错：
      - 显示缺执行文件`java.io.FileNotFoundException: Could not locate Hadoop executable:`根据[提示](https://wiki.apache.org/hadoop/WindowsProblems)在该网页的指定位置下载winutils.exe以及hadoop.dll，放在hadoop目录的bin下。至于要不要设置hadoop的环境变量，如果这样还不行就设置一下。不过GitHub没有3.2.0的，我发现[3.1.0](https://github.com/s911415/apache-hadoop-3.1.0-winutils)也可以用，但是3.0.0不能。不用在C:/Windows/System32文件夹下也拷贝一个hadoop.dll（我看好多网页上要加这个，我没加也顺利运行）。
3. 本人在Ubuntu下安装。
## Ubuntu下安装
1. 利用ubuntu自带的virtualBox，安装一个Ubuntu后设置启动模式为3，即命令行启动，不过命令行中文乱码，可以考虑[切换英文](https://www.cnblogs.com/york-hust/archive/2012/03/27/2419582.html)。还有一个问题是后面遇到的，一般/usr存放的是用户安装的软件，类似Windows的programFiles文件，我当初为了方便，给这个文件加了777的权限，本来默认755，[导致virtualBOX打不开](https://www.virtualbox.org/ticket/16759)，后来改回来后就可以了。如果非要这个777的权限，考虑[chmod +t](http://www.360doc.com/content/14/0216/20/13327838_353033771.shtml)这个方式
2. 安装好虚拟机后，首先安装Java。[可以选择命令行安装](https://blog.csdn.net/u012707739/article/details/78489833)，即`sudo apt-get install openjdk-8-jdk`。但是安装后`echo $JAVA_HOME`打印空，因为下面需要Java路径，[解决方案](https://www.cnblogs.com/jasonzeng/p/8302171.html),一般安装，查看Java位置`which java`，打印Java路径`ls -l /etc/alternatives/java`
2. 复制三台作为集群。每台设备固定IP地址，在主节点上加入这些IP地址映射/etc/hosts。
   - [固定IP地址](https://zhuanlan.zhihu.com/p/40405167)：
     - 首先要在VirtualBox界面-设置-网络中，选择Host-Only然后设置网卡。中途可能遇到的问题是选择仅主机网络时，界面名称也就是网卡没法选择：点击右上角的全局工具，新建就行了。要三个虚拟机，新建1个网卡，IP地址为固定的地址，但三个在选择界面名称时都选择同一个网卡（否则虚拟机之间无法通信）。另外，网卡地址一定不能跟三个IP地址相同。
     - 然后进虚拟机，配置固定IP。
   - ssh免密访问：A访问B，那么A将自己生成的.pub放到B的authorized_keys里面。这里直接在root用户下创建authorized_keys文件，然后写进去。因为后面用的是[root用户启动](https://blog.csdn.net/sinat_35820101/article/details/78393088)，因此不要用自己的用户，使用前root密码记得改，百度，改好后就不管了，否则就算密码对，也登不进去。不过我之前在Gitlab中有本地生产=成过ssh，现在在虚拟机中可以直接用。
     
      >网卡1采用Host-Only模式是为了给虚拟机设置一个固定IP，让主机与虚拟机网络相通。网卡2采用网络地址转换（NAT）模式，是为虚拟机配置一个上网的网卡。


2. [在一台主机配置Namenode](https://github.com/vbay/CSDN-CODE/tree/master/Hadoop-3.1.0-Fully-Distributed-Operation)：
   - `/etc/hadoop/hadoop-env.sh`中,HDFS守护程序是NameNode，SecondaryNameNode和DataNode。YARN守护程序是ResourceManager，NodeManager和WebAppProxy.我只想用hdfs,所以只配这么几个:
     - 配置HADOOP_PID_DIR（存储守护进程的进程标识文件的目录）、
     - HADOOP_LOG_DIR（存储守护程序日志文件的目录）、
     - 配置Namenode为了使其能够并行回收垃圾`export HDFS_NAMENODE_OPTS="-XX:+UseParallelGC -Xmx4g"`
     
            export JAVA_HOME=/java/jdk1.8.0_172/   # java地址
            export HDFS_NAMENODE_OPTS="-XX:+UseParallelGC -Xmx4g"
   - `/etc/profile.d`中（是计算机的/etc），我自己新建了一个.sh文件，加入Hadoop路径：
   
            HADOOP_HOME=/home/mi/Hadoop/hadoop-3.2.0/
            export HADOOP_HOME
   - core-site.xml
   
            <property>
               <name>fs.defaultFS</name>
               <value>hdfs://xu1:9000</value><!--	NameNode URI-->
           </property>

           <property>
               <name>io.file.buffer.size</name><!--Size of read/write buffer used in SequenceFiles.-->
               <value>131072</value>
           </property>
   - hdfs-site.xml
    
            <!-- Configurations for NameNode: -->
            <property>
              <name>dfs.namenode.name.dir</name>
              <value>/var/lib/hadoop/hdfs/name/</value><!--NameNode持久存储命名空间和事务日志的本地文件系统上的路径。-->
            </property>

            <property>
              <name>dfs.blocksize</name>
              <value>268435456</value>
            </property>

            <property>
              <name>dfs.namenode.handler.count	</name>
              <value>100</value>
            </property>
   - yarn-site.xml
    
            <property>
                   <name>yarn.resourcemanager.hostname</name><!--主机名-->
                   <value>servera</value>
           </property>
           <!-- 配置外网只需要替换外网ip为真实ip，否则默认为 localhost:8088。如果设置，则覆盖yarn.resourcemanager.hostname中设置的主机名。 -->
           <!-- <property>
                   <name>yarn.resourcemanager.webapp.address</name>
                   <value>外网ip:8088</value>
           </property> -->
            <!-- Configurations for NodeManager:需要为Map Reduce应用程序设置的随机服务。 -->
           <property>
                   <name>yarn.nodemanager.aux-services</name>
                   <value>mapreduce_shuffle</value>
           </property>
    - mapred-site.xml
     
             <property>
             <name>mapreduce.framework.name</name>
             <value>yarn</value>
             </property>
    - workers.加入节点主机名
    - 之后，最好不要看中文文档，英文翻译成中文都要好一点。另外，[文档版本记得匹配](https://hadoop.apache.org/docs/r3.2.0/hadoop-project-dist/hadoop-common/ClusterSetup.html)
    - 之后
    
             # 新建rcmd_default文件
             # 设置rcmd_default入口，避免出现 master: rcmd: socket: Permission denied
             sudo sh -c "echo "ssh" > /etc/pdsh/rcmd_default"
3. 启动命令，在Hadoop的目录下，当然也可以写完整路径
   - 第一次启动HDFS时，必须对其进行格式化。将新的分布式文件系统格式化为hdfs：`sudo bin/hdfs namenode -format xu-cluster`
   - 在指定节点上使用以下命令以hdfs启动HDFS NameNode ：`sudo bin/hdfs --daemon start namenode`
   - 在每个指定节点上使用以下命令启动HDFS DataNode作为hdfs：`sudo bin/hdfs --daemon start datanode`
   - 启动所有HDFS进程：

## 基本概念
1. 数据采集之后，该如何存储？HDFS，TFS等分布式文件存储系统.HDFS适用于一次写入多次查询的情况，不支持并发写情况，小文件不合适。
2. HDFS，按数据块存储，数据块作为一个抽象块而不是整个文件作为基本单元，默认大小64M：
   - NameNode:主节点。一个NameNode，多个DataNode。
      - 管理文件系统的命名空间，存放文件元数据
      - 维护文件系统的所有文件和目录，文件与数据块的映射
      - 记录着每个文件中各个块所在的数据节点的信息。
   - DataNode：从节点。
      - 存储并检索数据块
      - 向NameNode更新所存储块的列表
        >HDFS不适合大量小文件存储、不适合并发写入、不支持文件随机修改、不支持随机读等低延时的访问方式。
   - [HDFS写流程](https://www.cnblogs.com/LeonNew/p/5489462.html)：
      - 初始化FileSystem。客户端向NameNode发起写数据请求，客户端调用create()来创建文件
      - FileSystem用RPC调用元数据节点，在文件系统的命名空间中创建一个新的文件，元数据节点首先确定文件原来不存在，并且客户端有创建文件的权限，然后创建新文件。然后返回DFSOutputStream，用于客户端写数据。
      - client写入数据时，DFSOutputStream将数据分成一个个的包，写入内部队列，称为数据队列data queue。数据流Data Streamer负责处理数据队列，根据适合的DataNode的列表要求NameNode分配适合的新块来存储数据副本。然后将分配的DataNode放在一个pipeline里，Data Streamer将数据块写入pipeline中的第一个数据节点，第一个数据节点将数据块发送给第二个数据节点，第二个数据节点将数据发送给第三个数据节点。
      - DFSOutputStream内部存在确认队列ack queue，等待pipeline中的DataNode告知数据已经写入成功。
      
                FileSystem fs=null;

                @Before
                public void init() throws Exception{
                     fs= FileSystem.get(new URI("hdfs://hadoop01:9000"),new Configuration(),"root");
                }

                @Test
                public void testUpLoad() throws Exception{
                     OutputStream out = fs.create(new Path("/Xshellqqq"));//发起写请求
                     InputStream in = new FileInputStream(new File("c:/Xshell-5.0.1337p.exe"));//将本地文件写入流
                     IOUtils.copyBytes(in, out, 4096, true);//将文件流in写入out后，即上面第三步开始执行
                }

                @Test
                public void testCopyFromLocalFile() throws IllegalArgumentException, IOException{
                     fs.copyFromLocalFile(new Path("c:/Xshell-5.0.1337p.exe"), new Path("/1132/Xshellaaa"));
                }
   - HDFS读流程：
      - 初始化FileSystem.
      - 客户端向NameNode发起数据请求,即客户端(client)用FileSystem的open()函数打开文件
      - FileSystem用RPC调用元数据节点，得到文件的数据块信息，对于每一个数据块，元数据节点返回保存数据块的数据节点的地址。
      - FileSystem返回FSDataInputStream给客户端，用来读取数据，客户端调用stream的read()函数开始读取数据。
      - 而DFSInputStream连接保存此文件第一个数据块的最近的数据节点，data从数据节点读到客户端(client)。即NameNode找出距离最近的DataNode节点信息，客户端从DataNode分块下载文件。
      - 当此数据块读取完毕时，DFSInputStream关闭和此数据节点的连接，然后连接此文件下一个数据块的最近的数据节点。
      
                public static void main(String[] args) throws Exception {
                  
                  FileSystem fs = FileSystem.get(new URI("hdfs://hadoop01:9000"),new Configuration());
                  InputStream in = fs.open(new Path("/dianying.mp4"));  //open后，FileSystem返回FSDataInputStream给客户端
                  OutputStream out = new  FileOutputStream(new File("c:/dianying.mp4"));//客户端自己新建一个输出流用于接收服务器的数据
                  IOUtils.copyBytes(in, out, 4096, true);		
               }
      
      
3. HDFS常用命令：
   - copyFromLocal从本地拷贝到HDFS文件系统.类似的copyToLocal.
   - get下载文件，put上传文件。
4. MapReduce(分布式计算系统)
   - MapReduce是处理数据的编程模型。
   - MapReduce分为两个阶段，map阶段和reduce阶段，map阶段将原始数据进行过滤操作，以键/值对的方式输出，map阶段的输出是reduce阶段的输入，reduce阶段对数据处理后输出最终的结果。
   - MapReduce处理的数据文件保存在HDFS上，并且最终的计算结果同样会保存到HDFS上。
5. [早期MapReduce](https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-yarn/),由一个JobTracker和多个TaskTracker组成
   - 首先用户程序 (JobClient) 提交了一个 job，job 的信息会发送到 Job Tracker 中，Job Tracker 是 Map-reduce 框架的中心，他需要与集群中的机器定时通信 (heartbeat), 需要管理哪些程序应该跑在哪些机器上，需要管理所有 job 失败、重启等操作。
   - TaskTracker 是 Map-reduce 集群中每台机器都有的一个部分，他做的事情主要是监视自己所在机器的资源情况。
   - 流程：TaskTracker执行JobTracker指定的任务。当一个Job(数据分析作业)提交时，JobTracker接收到提交的任作业后，将作业执行需要的配置信息和其他数据信息分发给相应的TaskTracker。同时要调度任务并监控TaskTracker的执行。
6. [YARN](https://github.com/sunnyandgood/BigData/blob/master/HDFS/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0.md)(分布式计算系统)
   - YARN将JobTracker分为两个组件：ResourceManager和ApplicationMaster。
   - ResourceManager进程运行在主节点上，作为集群资源的管理者和总调度，不在需要关心应用程序的执行和监控。ApplicationMaster进程运行在从节点上，主要管理应用程序的执行和应用程序的生命周期，当应用程序执行结束，ApplicationMaster的生命周期结束。
   - 客户端发出应用程序执行请求，ResourceManager会创建与应用程序对应的ApplicationMaster实例。
   
7. [mapReduce](https://github.com/sunnyandgood/BigData/blob/master/MapReduce/MapReduce%E5%8E%9F%E7%90%86.md)
8. [Partitioner](https://www.jianshu.com/p/0f85d9688ebe)编程:就是将map的输出找reducer。将mapper（如果使用了combiner的话就是combiner）输出的key/value拆分为分片（shard），每个reducer对应一个分片。默认情况下，MR调用Hashpartitioner类，如果程序员编写了自己的partition类，那么就使用自己编写的partition编程进行数据分，以达到map阶段的数据分区切片，从而防止reduce阶段的数据倾斜问题，实现负载均衡。
8. [Combiner](https://www.cnblogs.com/edisonchou/p/4297786.html)编程
   - Combiner最基本是实现本地key的聚合，对map输出的key排序，value进行迭代。

           map: (K1, V1) → list(K2, V2) 
           combine: (K2, list(V2)) → list(K2, V2) 
           reduce: (K2, list(V2)) → list(K3, V3)
   - Combiner还有本地reduce功能（其本质上就是一个reduce），例如Hadoop自带的wordcount的例子和找出value的最大值的程序，combiner和reduce完全一致，如下所示：
   
          map: (K1, V1) → list(K2, V2) 
          combine: (K2, list(V2)) → list(K3, V3) 
          reduce: (K3, list(V3)) → list(K4, V4)
9. [zookeeper](https://www.cnblogs.com/ultranms/p/9585191.html)
   - 功能：
     - 配置管理
     - 名字服务。类似于ip与域名的映射，当服务多的时候，名字服务提供：‘服务地址’与‘用户都熟知的访问点’之间的映射。
     - 分布式锁。共享锁在同一个进程中很容易实现，但是在跨进程或者在不同 Server 之间就不好实现了。
     - 集群管理。有机器死亡或新增，集群中其他机器需要感知到这种变化，然后根据这种变化做出对应的决策。
   - Zookeeper设计目的:
     - 最终一致性：client不论连接到哪个Server，展示给它都是同一个视图，这是zookeeper最重要的性能。 
     - 可靠性：具有简单、健壮、良好的性能，如果消息被到一台服务器接受，那么它将被所有的服务器接受。 
     - 实时性：Zookeeper保证客户端将在一个时间间隔范围内获得服务器的更新信息，或者服务器失效的信息。但由于网络延时等原因，Zookeeper不能保证两个客户端能同时得到刚更新的数据，如果需要最新数据，应该在读数据之前调用sync()接口。 
     - 原子性：更新只能成功或者失败，没有中间状态。 
     - 顺序性：包括全局有序和偏序两种：全局有序是指如果在一台服务器上消息a在消息b前发布，则在所有Server上消息a都将在消息b前被发布；偏序是指如果一个消息b在消息a后被同一个发送者发布，a必将排在b前面。
     - 等待无关（wait-free）：慢的或者失效的client不得干预快速的client的请求，使得每个client都能有效的等待。感觉不是IFIO.
   - 针对Hadoop.引入支持Active/StandBy 模式的HA架构。过程
     - 创建锁节点:全部的ResourceManager在启动的时候会竞争写一个/yarn-leader-election/pseudo-yarn-rm-cluster节点(暂时节点)。创建成功的ResourceManager节点变成Active节点，其它的切换为StandBy
     - 注冊Watcher节点:全部的standby的ResourceManager节点会向/yarn-leader-election/pseudo-yarn-rm-cluster节点注冊一个Watcher
     - 主备切换:当Active的ResourceManager节点出现异常或挂掉时。起在zookeeper上创建的暂时节点也会被删除。standy的ResourceManager节点检測到该节点发生变化时，会又一次发起竞争，直到产生一个Active节点
     - 假设集群中存在两个ResourceManager节点RM1,RM2,在通过竞争操作后。RM1变成了Active后。假设某个时间段RM1因为资源损耗比較严重。产生了假死的现象。此时的zookeeper会以为RM1这台机器出现了故障。于是发起新一轮的竞选，选了RM2作为Active,在RM2变成Active后，RM1恢复了服务可是它任然以为自己是Active的，此时就出现了两个Active的情况。这样的情况又称为“脑裂”，为了解决这样的问题能够在创建根节点的时候引入ACL控制，这样的话当RM1恢复后尝试更新数据时，会发现相应的节点必须提供RM2的ACL信息才干够更新相应的数据
10. [Sqoop](https://github.com/sunnyandgood/BigData/blob/master/Sqoop/sqoop%E7%AE%80%E4%BB%8B.md)
11. [HBase](https://dzone.com/articles/understanding-hbase-and-bigtab).数据可以存储在HDFS中，同时也存储在HBase中(两者的数据可以稍有不同，比如一个数据比较全，另外一个根据需求，可以少一些)，如果需要实时查询某些数据时，使用HBase;在进行Hive统计，或者MapReduce运算时，使用HDFS数据。即用Hadoop作为静态数据仓库，HBase作为数据存储，放那些进行一些操作会改变的数据。
12. [HIVE](https://github.com/sunnyandgood/BigData/blob/master/Hive/Hive%E7%AE%80%E4%BB%8B.md)：Hive的表对应HDFS的目录（或文件夹）；Hive表中的数据对应HDFS的文件。执行流程：HiveQL 通过命令行或者客户端提交，经过 Compiler 编译器，运用 MetaStore 中的元数 据进行类型检测和语法分析，生成一个逻辑方案(Logical Plan)，然后通过的优化处理，产生 一个 MapReduce 任务。架构如下图：
    - 用户接口，包括 CLI，JDBC/ODBC，WebUI。CLI，即Shell命令行；JDBC/ODBC 是 Hive 的Java，与使用传统数据库JDBC的方式类似；WebGUI是通过浏览器访问 Hive
    - 元数据存储，Hive 将元数据存储在数据库中(metastore)，目前只支持 mysql、oracle、derby。Hive 中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等
    - 解释器、编译器、优化器完成 HQL 查询语句从词法分析、语法分析、编译、优化以及查询计划（plan）的生成。生成的查询计划存储在 HDFS 中，并在随后有 MapReduce 调用执行
    - Hive 的数据存储在 HDFS 中，大部分的查询由 MapReduce 完成（包含 * 的查询，比如 select * from table 不会生成 MapRedcue 任务）

      <img src="https://github.com/xuzhuang1996/MyJava/blob/master/img/BD/Hive的系统架构.png" width=50% height=50% />
    - Hive的数据模型
      - 内部表Table。在 HDFS 中表现所属 database 目录下一个文件夹，每一个 Table 在 Hive 中都有一个相应的目录存储数据。例如，一个表 test，它在 HDFS 中的路径为：/ warehouse/test。 warehouse是在 hive-site.xml 中由 ${hive.metastore.warehouse.dir} 指定的数据仓库的目录
      - external table外部表：与 Table 类似，指向已经在 HDFS 中存在的数据。加载数据和创建表同时完成，并不会移动到数据仓库目录中，只是与外部数据建立一个链接。当删除一个 外部表 时，仅删除该链接
      - Partition分区表在 HDFS 中表现为 table 目录下的子目录。如果你文件非常之大的话，采用分区表可以快过滤出按分区字段划分的数据
      
            例如：test表中包含 date 和 city 两个 Partition，
            则对应于date=20180729, city = bj 的 HDFS 子目录为：
            /warehouse/test/date=20130201/city=bj
            对应于date=20180729, city=sh 的HDFS 子目录为；
            /warehouse/test/date=20180729/city=sh
      - bucket桶表，是对数据进行哈希取值，然后放到不同文件中存储。在 HDFS 中表现为同一个表目录或者分区目录下根据某个字段的值进行 hash 散 列之后的多个文件
13. [Kylin](https://cloud.tencent.com/developer/article/1387322)的出现就是为了解决大数据系统中TB级别数据的数据分析需求，它提供Hadoop/Spark之上的SQL查询接口及多维分析(OLAP)能力以支持超大规模数据，它能在亚秒内查询巨大的Hive表。其核心是预计算，计算结果存在HBase中.
